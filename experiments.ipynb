{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm experimenting with an approach to improve how Spoken Dialog Systems (like OpenAI's Advanced Voice Mode) identify the end of a user's converational turn so the system can jump in appropriately without interrupting the user mid-thought.\n",
    "\n",
    "As a user, current systems appear to rely primarily on pause length from a Voice Activity Detector (VAD). So an `X` second pause mid-sentence is treated similarly to `X` seconds of silence at the end of a sentence.\n",
    "\n",
    "This notebook adds an end-of-turn prediction head to a Whisper model. This prediction head relies on both acoustic and linguistic information (both the Whisper encoder and the Whisper decoder). I will train it from multi-turn conversations between human speakers with labeled transitions. Implicitly, I assume humans do well at knowing when to jump in without interrupting ¯\\_(ツ)_/¯\n",
    "\n",
    "Candidate datasets for training: AMI Meeting Corpus, Switchboard, Fisher English Training Speech, DailyTalk, CALLHOME, CoVoST. If necessary, I could use diarazation tools like pyannotate-audio or pyAudioAnalysis to identify changes in the speaker.\n",
    "\n",
    "A production system might still use a VAD as a computationally cheap approach to identify pauses of at least 1 second, and then I'd call my updated model from this notebook for final end-of-turn detection and transcription only when there is some meaningful pause.\n",
    "\n",
    "This is an educational project to gain initial experience with audio models. I may be naively underestimating what others before me have done. I will nevertheless run incremental experiments as a learning experience. The main steps are:\n",
    "\n",
    "- [x] Download a Whisper model from HuggingFace hub and verify that I can run it on a trivial file\n",
    "- [ ] Inspect the architecture and plan how to integrate a prediction head\n",
    "- [ ] Overfit on a single sample to verify I can train\n",
    "- [ ] Train on a set of ~100 samples with some small number of validation samples to test infrastructure\n",
    "- [ ] Train on a larger sample to test if I can make something that broadly \"works\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Calling A Whisper Model\n",
    "\n",
    "I start with `whisper-tiny.en`. Later stages may use a large model like `whisper-large-v3` once I have the basic workflow wired up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/Users/dan/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My name is Dan and this is a test audio file.\n",
      "Time taken: 0.21962380409240723\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-tiny.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    " \n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "start = time()\n",
    "result = pipe(\"./test_data/test.wav\")\n",
    "print(result[\"text\"])\n",
    "print(f\"Time taken: {time() - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Pipeline and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type', 'task', 'model', 'tokenizer', 'feature_extractor', 'image_processor', 'processor', 'modelcard', 'framework', 'device', 'binary_output', 'prefix', 'generation_config', 'call_count']\n"
     ]
    }
   ],
   "source": [
    "# Whats' in the pipeline?\n",
    "print([step for step in pipe.__dict__.keys() if not step.startswith(\"_\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperFeatureExtractor {\n",
       "  \"chunk_length\": 30,\n",
       "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
       "  \"feature_size\": 80,\n",
       "  \"hop_length\": 160,\n",
       "  \"n_fft\": 400,\n",
       "  \"n_samples\": 480000,\n",
       "  \"nb_max_frames\": 3000,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"WhisperProcessor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline expects incoming audio at 16kHz. Preprocessor creates 80 dimensional features in 0.01second increments (the hop length of 160 means 160 frames/samples of audio are combined into a single input frame to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "WhisperForConditionalGeneration                         [1, 1500, 384]            --\n",
      "├─WhisperModel: 1-1                                     [1, 1500, 384]            --\n",
      "│    └─WhisperEncoder: 2-1                              [1, 1500, 384]            576,000\n",
      "│    │    └─Conv1d: 3-1                                 [1, 384, 3000]            92,544\n",
      "│    │    └─Conv1d: 3-2                                 [1, 384, 1500]            442,752\n",
      "│    │    └─ModuleList: 3-3                             --                        7,096,320\n",
      "│    │    └─LayerNorm: 3-4                              [1, 1500, 384]            768\n",
      "│    └─WhisperDecoder: 2-2                              [1, 6, 1, 64]             --\n",
      "│    │    └─Embedding: 3-5                              [1, 1, 384]               19,915,776\n",
      "│    │    └─WhisperPositionalEmbedding: 3-6             [1, 1, 384]               172,032\n",
      "│    │    └─ModuleList: 3-7                             --                        9,463,296\n",
      "│    │    └─LayerNorm: 3-8                              [1, 1, 384]               768\n",
      "├─Linear: 1-2                                           [1, 1, 51864]             19,915,776\n",
      "=========================================================================================================\n",
      "Total params: 57,676,032\n",
      "Trainable params: 57,100,032\n",
      "Non-trainable params: 576,000\n",
      "Total mult-adds (Units.MEGABYTES): 998.32\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.96\n",
      "Forward/backward pass size (MB): 258.64\n",
      "Params size (MB): 228.40\n",
      "Estimated Total Size (MB): 488.00\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "decoder_input_ids = torch.tensor([[processor.tokenizer.pad_token_id]]).to(device)\n",
    "print(summary(pipe.model, input_size=(1, 80, 3000), decoder_input_ids=decoder_input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding TurnEndClassifier to Whisper Model\n",
    "\n",
    "I add a TurnEndClassifier as an additional prediction head for the Whisper model. It's inputs are \n",
    "1. audio features from the encoder\n",
    "2. Semantic info from decoder\n",
    "\n",
    "## Alignment\n",
    "\n",
    "Each recording will be classified with a single prediction of whether it ends at a turn-end. The recording has many values from the encoder on the time dimension (100 samples per second) and many values from the decoder (1 hidden state per token).\n",
    "\n",
    "I compress the time and token dimensions from the encoder and decoder respectively into a 1d representation from each. I do these compressions with convolutional layers and then pooling layers (separate layers for the encoder and the decoder compression to 1d). These 1d vectors are concatenated, fed through a small feedforward network, and lead to a binary classification head indicating if this audio finishes with a turn-end.\n",
    "\n",
    "For training, I'll preprocess conversations to have many samples that are each 10 seconds of audio and that finish at a turn end (target is 1) or that don't (target is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class TurnEndClassifier(nn.Module):\n",
    "    def __init__(self, encoder_dim=384, decoder_dim=384, hidden_dim=64):\n",
    "        super(TurnEndClassifier, self).__init__()\n",
    "        print(f\"Initializing TurnEndClassifier with dims: encoder={encoder_dim}, decoder={decoder_dim}\")\n",
    "\n",
    "        # Linear layers to reduce the dimensions of the encoder and decoder outputs.\n",
    "        # This is a temporary hack. In theory, hidden_dim could be larger than whisper_hidden_dim. Or encoder and decoder could have different dimensions.\n",
    "        self.encoder_reduce = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.decoder_reduce = nn.Linear(decoder_dim, hidden_dim)\n",
    "        \n",
    "        # Encoder processing layers\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.encoder_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Decoder processing layers\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim , hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_outputs):\n",
    "        print(f\"TurnEndClassifier input shapes:\")\n",
    "        print(f\"- encoder_outputs: {encoder_outputs.shape}\")  # [batch, time, features]\n",
    "        print(f\"- decoder_outputs: {decoder_outputs.shape}\")  # [batch, seq_len, features]\n",
    "\n",
    "        # Reduce feature dimension first (more efficient for subsequent conv layers)\n",
    "        # [batch, time, whisper_hidden_dim] -> [batch, time, hidden_dim]\n",
    "        encoder_outputs = self.encoder_reduce(encoder_outputs)\n",
    "        \n",
    "        # [batch, seq_len, whisper_hidden_dim] -> [batch, seq_len, hidden_dim]\n",
    "        decoder_outputs = self.decoder_reduce(decoder_outputs)\n",
    "        \n",
    "        # Process encoder outputs\n",
    "        # [batch, time, hidden_dim] -> [batch, hidden_dim, time]\n",
    "        encoder_outputs = encoder_outputs.transpose(1, 2)  # Conv1d expects channels first\n",
    "        encoder_features = self.encoder_conv(encoder_outputs)  # Now working with reduced dimensions\n",
    "        encoder_features = self.encoder_pool(encoder_features).squeeze(-1)  # -> [batch, hidden_dim]\n",
    "        \n",
    "        # Process decoder outputs\n",
    "        # [batch, seq_len, hidden_dim] -> [batch, hidden_dim, seq_len]\n",
    "        decoder_outputs = decoder_outputs.transpose(1, 2)  # Conv1d expects channels first\n",
    "        decoder_features = self.decoder_conv(decoder_outputs)  # Now working with reduced dimensions\n",
    "        decoder_features = self.decoder_pool(decoder_features).squeeze(-1)  # -> [batch, hidden_dim]\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        # [batch, hidden_dim] + [batch, hidden_dim] -> [batch, 2*hidden_dim]\n",
    "        combined_features = torch.cat((encoder_features, decoder_features), dim=1)\n",
    "        # [batch, 2*hidden_dim] -> [batch, 1]\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model\n",
    "Now we build the custom model that is a Whisper Model with the extra prediction head. Our goals include:\n",
    "1. Allow training the layers in the TurnEndClassifier prediction head while keeping all other layers frozen\n",
    "2. Allow embedding this in a pipeline that is as similar as possible to the pipeline used at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomWhisperModel(WhisperForConditionalGeneration):\n",
    "    def __init__(self, config, turn_end_classifier=None):\n",
    "        super().__init__(config)\n",
    "        self.turn_end_classifier = turn_end_classifier # if turn_end_classifier is None, we'll set it later\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, turn_end_classifier, *args, **kwargs):\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n",
    "        model.turn_end_classifier = turn_end_classifier\n",
    "        return model\n",
    "\n",
    "    def forward_with_turn_end(\n",
    "        self,\n",
    "        input_features=None,\n",
    "        decoder_input_ids=None,\n",
    "        **kwargs  # Catch all other args to pass through\n",
    "    ):\n",
    "        assert self.turn_end_classifier is not None, \"TurnEndClassifier must be set before calling forward_with_turn_end\"\n",
    "        \n",
    "        # whisper_outputs is a Seq2SeqLMOutput object with `logits` for each token, encoder_last_hidden_state, and decoder_hidden_states, decoder_hidden_states, encoder_hidden_states and some other attributes. It's a type of namedtuple.\n",
    "        whisper_outputs = super().forward(\n",
    "            input_features=input_features,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            output_hidden_states=True,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Get output of last hidden layer from both encoder and decoder.\n",
    "        encoder_hidden_states = whisper_outputs.encoder_last_hidden_state\n",
    "        decoder_hidden_states = whisper_outputs.decoder_hidden_states[-1]\n",
    "\n",
    "        # Get turn-end predictions\n",
    "        turn_end_predictions = self.turn_end_classifier(\n",
    "            encoder_hidden_states,\n",
    "            decoder_hidden_states\n",
    "        )\n",
    "        \n",
    "        return whisper_outputs, turn_end_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the CustomWhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TurnEndClassifier with dims: encoder=384, decoder=384\n",
      "TurnEndClassifier input shapes:\n",
      "- encoder_outputs: torch.Size([2, 1500, 384])\n",
      "- decoder_outputs: torch.Size([2, 20, 384])\n",
      "Output type: <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
      "\n",
      "Available attributes:\n",
      "['clear', 'copy', 'cross_attentions', 'decoder_attentions', 'decoder_hidden_states', 'encoder_attentions', 'encoder_hidden_states', 'encoder_last_hidden_state', 'fromkeys', 'get', 'items', 'keys', 'logits', 'loss', 'move_to_end', 'past_key_values', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n",
      "\n",
      "Key shapes:\n",
      "- logits for word probabilities: torch.Size([2, 20, 51864])\n",
      "- turn_end_predictions: tensor([[0.5006],\n",
      "        [0.5208]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test_custom_whisper_model():\n",
    "    # 1. Create fake data\n",
    "    batch_size = 2\n",
    "    sequence_length = 3000\n",
    "    encoder_dim = 384  # Whisper tiny dimension\n",
    "    decoder_seq_length = 20\n",
    "    \n",
    "    fake_input_features = torch.randn(batch_size, 80, sequence_length)\n",
    "    fake_decoder_input_ids = torch.randint(0, 100, (batch_size, decoder_seq_length))\n",
    "    \n",
    "    # 2. Initialize models\n",
    "    turn_end_classifier = TurnEndClassifier(\n",
    "        encoder_dim=encoder_dim,\n",
    "        decoder_dim=encoder_dim,\n",
    "        hidden_dim=64\n",
    "    )\n",
    "    \n",
    "    model = CustomWhisperModel.from_pretrained(\n",
    "        \"openai/whisper-tiny.en\",\n",
    "        turn_end_classifier=turn_end_classifier\n",
    "    )\n",
    "    \n",
    "    # 3. Run forward pass\n",
    "\n",
    "    whisper_outputs, turn_end_predictions = model.forward_with_turn_end(\n",
    "        input_features=fake_input_features,\n",
    "        decoder_input_ids=fake_decoder_input_ids,\n",
    "    )\n",
    "    print(f\"Output type: {type(whisper_outputs)}\")\n",
    "    print(\"\\nAvailable attributes:\")\n",
    "    print([attr for attr in dir(whisper_outputs) if not attr.startswith('_')])\n",
    "    \n",
    "    print(\"\\nKey shapes:\")\n",
    "    print(f\"- logits for word probabilities: {whisper_outputs.logits.shape}\")\n",
    "    print(f\"- turn_end_predictions: {turn_end_predictions}\")\n",
    "    \n",
    "        \n",
    "test_custom_whisper_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put In A Custom Pipeline\n",
    "\n",
    "Once we have the CustomWhisperModel working, I will create a TurnEndPipeline(AutomaticSpeechRecognitionPipeline) class. It will take a .wav file as input and return both the text and turn_end_probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iw_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
