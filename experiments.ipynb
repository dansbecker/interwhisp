{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm experimenting with an approach to improve how Spoken Dialog Systems (like OpenAI's Advanced Voice Mode) identify the end of a user's converational turn so the system can jump in appropriately without interrupting the user mid-thought.\n",
    "\n",
    "As a user, current systems appear to rely primarily on pause length from a Voice Activity Detector (VAD). So an `X` second pause mid-sentence is treated similarly to `X` seconds of silence at the end of a sentence.\n",
    "\n",
    "This notebook adds an end-of-turn prediction head to a Whisper model. This prediction head relies on both acoustic and linguistic information (both the Whisper encoder and the Whisper decoder). I will train it from multi-turn conversations between human speakers with labeled transitions. Implicitly, I assume humans do well at knowing when to jump in without interrupting ¯\\_(ツ)_/¯\n",
    "\n",
    "Candidate datasets for training: AMI Meeting Corpus, Switchboard, Fisher English Training Speech, DailyTalk, CALLHOME, CoVoST. If necessary, I could use diarazation tools like pyannotate-audio or pyAudioAnalysis to identify changes in the speaker.\n",
    "\n",
    "A production system might still use a VAD as a computationally cheap approach to identify pauses of at least 1 second, and then I'd call my updated model from this notebook for final end-of-turn detection and transcription only when there is some meaningful pause.\n",
    "\n",
    "This is an educational project to gain initial experience with audio models. I may be naively underestimating what others before me have done. I will nevertheless run incremental experiments as a learning experience. The main steps are:\n",
    "\n",
    "- [x] Download a Whisper model from HuggingFace hub and verify that I can run it on a trivial file\n",
    "- [x] Inspect the architecture and plan how to integrate a prediction head\n",
    "- [ ] Overfit on a single sample to verify I can train\n",
    "- [ ] Train on a set of ~100 samples with some small number of validation samples to test infrastructure\n",
    "- [ ] Train on a larger sample to test if I can make something that broadly \"works\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Calling A Whisper Model\n",
    "\n",
    "I start with `whisper-tiny.en`. Later stages may use a large model like `whisper-large-v3` once I have the basic workflow wired up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/Users/dan/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['text'])\n",
      " My name is Dan and this is a test audio file.\n",
      "Time taken: 0.21422886848449707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" Okay, this is going to be a test of recordings that are longer than 30 seconds so that I can test how well this model handles multi chunk. Audio files. So each chunk is going to be 30 seconds. If you have a recording this longer than 30 seconds, it's going to be processed as multiple chunks. And then each chunk is going to be handled by the large-engaged model to make a transcription. And then we're going to concatenate those various transcriptions. .\", 'chunks': [{'timestamp': (0.0, 15.0), 'text': ' Okay, this is going to be a test of recordings that are longer than 30 seconds so that I can test how well this model handles multi chunk.'}, {'timestamp': (15.0, 0.0), 'text': ''}, {'timestamp': (7.0, 14.0), 'text': ' Audio files. So each chunk is going to be 30 seconds.'}, {'timestamp': (14.0, 18.0), 'text': \" If you have a recording this longer than 30 seconds, it's going to be processed as multiple chunks.\"}, {'timestamp': (18.0, 23.0), 'text': ' And then each chunk is going to be handled by the large-engaged model to make a transcription.'}, {'timestamp': (23.0, 28.0), 'text': \" And then we're going to concatenate those various transcriptions.\"}, {'timestamp': (28.0, 0.0), 'text': ''}, {'timestamp': (2.0, None), 'text': ' .'}]}\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-tiny.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    " \n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "start = time()\n",
    "result = pipe(\"./test_data/test.wav\")\n",
    "print(f\"Keys: {result.keys() if isinstance(result, dict) else 'not a dict'}\")\n",
    "print(result[\"text\"])\n",
    "print(f\"Time taken: {time() - start}\")\n",
    "\n",
    "long_audio_file = \"./test_data/multi-chunk-test.wav\"\n",
    "result2 = pipe(long_audio_file, return_timestamps=True)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Pipeline and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type', 'task', 'model', 'tokenizer', 'feature_extractor', 'image_processor', 'processor', 'modelcard', 'framework', 'device', 'binary_output', 'prefix', 'generation_config', 'call_count']\n"
     ]
    }
   ],
   "source": [
    "# Whats' in the pipeline?\n",
    "print([step for step in pipe.__dict__.keys() if not step.startswith(\"_\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperFeatureExtractor {\n",
       "  \"chunk_length\": 30,\n",
       "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
       "  \"feature_size\": 80,\n",
       "  \"hop_length\": 160,\n",
       "  \"n_fft\": 400,\n",
       "  \"n_samples\": 480000,\n",
       "  \"nb_max_frames\": 3000,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"WhisperProcessor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline expects incoming audio at 16kHz. Preprocessor creates 80 dimensional features in 0.01second increments (the hop length of 160 means 160 frames/samples of audio are combined into a single input frame to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "WhisperForConditionalGeneration                         [1, 1500, 384]            --\n",
      "├─WhisperModel: 1-1                                     [1, 1500, 384]            --\n",
      "│    └─WhisperEncoder: 2-1                              [1, 1500, 384]            576,000\n",
      "│    │    └─Conv1d: 3-1                                 [1, 384, 3000]            92,544\n",
      "│    │    └─Conv1d: 3-2                                 [1, 384, 1500]            442,752\n",
      "│    │    └─ModuleList: 3-3                             --                        7,096,320\n",
      "│    │    └─LayerNorm: 3-4                              [1, 1500, 384]            768\n",
      "│    └─WhisperDecoder: 2-2                              [1, 6, 1, 64]             --\n",
      "│    │    └─Embedding: 3-5                              [1, 1, 384]               19,915,776\n",
      "│    │    └─WhisperPositionalEmbedding: 3-6             [1, 1, 384]               172,032\n",
      "│    │    └─ModuleList: 3-7                             --                        9,463,296\n",
      "│    │    └─LayerNorm: 3-8                              [1, 1, 384]               768\n",
      "├─Linear: 1-2                                           [1, 1, 51864]             19,915,776\n",
      "=========================================================================================================\n",
      "Total params: 57,676,032\n",
      "Trainable params: 57,100,032\n",
      "Non-trainable params: 576,000\n",
      "Total mult-adds (Units.MEGABYTES): 998.32\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.96\n",
      "Forward/backward pass size (MB): 258.64\n",
      "Params size (MB): 228.40\n",
      "Estimated Total Size (MB): 488.00\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "decoder_input_ids = torch.tensor([[processor.tokenizer.pad_token_id]]).to(device)\n",
    "print(summary(pipe.model, input_size=(1, 80, 3000), decoder_input_ids=decoder_input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding TurnEndClassifier to Whisper Model\n",
    "\n",
    "I add a TurnEndClassifier as an additional prediction head for the Whisper model. It's inputs are \n",
    "1. audio features from the encoder\n",
    "2. Semantic info from decoder\n",
    "\n",
    "## Alignment\n",
    "\n",
    "Each recording will be classified with a single prediction of whether it ends at a turn-end. The recording has many values from the encoder on the time dimension (100 samples per second) and many values from the decoder (1 hidden state per token).\n",
    "\n",
    "I compress the time and token dimensions from the encoder and decoder respectively into a 1d representation from each. I do these compressions with convolutional layers and then pooling layers (separate layers for the encoder and the decoder compression to 1d). These 1d vectors are concatenated, fed through a small feedforward network, and lead to a binary classification head indicating if this audio finishes with a turn-end.\n",
    "\n",
    "For training, I'll preprocess conversations to have many samples that are each 10 seconds of audio and that finish at a turn end (target is 1) or that don't (target is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class TurnEndClassifier(nn.Module):\n",
    "    def __init__(self, encoder_dim=384, decoder_dim=384, hidden_dim=64):\n",
    "        super(TurnEndClassifier, self).__init__()\n",
    "\n",
    "        # Linear layers to reduce the dimensions of the encoder and decoder outputs.\n",
    "        # This is a temporary hack. In theory, hidden_dim could be larger than whisper_hidden_dim. Or encoder and decoder could have different dimensions.\n",
    "        self.encoder_reduce = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.decoder_reduce = nn.Linear(decoder_dim, hidden_dim)\n",
    "        \n",
    "        # Encoder processing layers\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.encoder_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Decoder processing layers\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim , hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_outputs):\n",
    "        print(f\"TurnEndClassifier input shapes:\")\n",
    "        print(f\"- encoder_outputs: {encoder_outputs.shape}\")  # [batch, time, features]\n",
    "        print(f\"- decoder_outputs: {decoder_outputs.shape}\")  # [batch, seq_len, features]\n",
    "\n",
    "        # Reduce feature dimension first (more efficient for subsequent conv layers)\n",
    "        # [batch, time, whisper_hidden_dim] -> [batch, time, hidden_dim]\n",
    "        encoder_outputs = self.encoder_reduce(encoder_outputs)\n",
    "        \n",
    "        # [batch, seq_len, whisper_hidden_dim] -> [batch, seq_len, hidden_dim]\n",
    "        decoder_outputs = self.decoder_reduce(decoder_outputs)\n",
    "        \n",
    "        # Process encoder outputs\n",
    "        # [batch, time, hidden_dim] -> [batch, hidden_dim, time]\n",
    "        encoder_outputs = encoder_outputs.transpose(1, 2)  # Conv1d expects channels first\n",
    "        encoder_features = self.encoder_conv(encoder_outputs)  # Now working with reduced dimensions\n",
    "        encoder_features = self.encoder_pool(encoder_features).squeeze(-1)  # -> [batch, hidden_dim]\n",
    "        \n",
    "        # Process decoder outputs\n",
    "        # [batch, seq_len, hidden_dim] -> [batch, hidden_dim, seq_len]\n",
    "        decoder_outputs = decoder_outputs.transpose(1, 2)  # Conv1d expects channels first\n",
    "        decoder_features = self.decoder_conv(decoder_outputs)  # Now working with reduced dimensions\n",
    "        decoder_features = self.decoder_pool(decoder_features).squeeze(-1)  # -> [batch, hidden_dim]\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        # [batch, hidden_dim] + [batch, hidden_dim] -> [batch, 2*hidden_dim]\n",
    "        combined_features = torch.cat((encoder_features, decoder_features), dim=1)\n",
    "        # [batch, 2*hidden_dim] -> [batch, 1]\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model\n",
    "Now we build the custom model that is a Whisper Model with the extra prediction head. Our goals include:\n",
    "1. Allow training the layers in the TurnEndClassifier prediction head while keeping all other layers frozen\n",
    "2. Allow embedding this in a pipeline that reuses parts of the Whisper pipeline (e.g. for preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomWhisperModel(WhisperForConditionalGeneration):\n",
    "    def __init__(self, config, turn_end_classifier=None):\n",
    "        super().__init__(config)\n",
    "        self.turn_end_classifier = turn_end_classifier # if turn_end_classifier is None, we'll set it later\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, turn_end_classifier, *args, **kwargs):\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n",
    "        model.turn_end_classifier = turn_end_classifier\n",
    "        return model\n",
    "\n",
    "    def forward_with_turn_end(\n",
    "        self,\n",
    "        input_features=None,\n",
    "        decoder_input_ids=None,\n",
    "        **kwargs  # Catch all other args to pass through\n",
    "    ):\n",
    "        assert self.turn_end_classifier is not None, \"TurnEndClassifier must be set before calling forward_with_turn_end\"\n",
    "        \n",
    "        # whisper_outputs is a Seq2SeqLMOutput object with `logits` for each token, encoder_last_hidden_state, and decoder_hidden_states, decoder_hidden_states, encoder_hidden_states and some other attributes. It's a type of namedtuple.\n",
    "        whisper_outputs = super().forward(\n",
    "            input_features=input_features,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            output_hidden_states=True,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Get output of last hidden layer from both encoder and decoder.\n",
    "        encoder_hidden_states = whisper_outputs.encoder_last_hidden_state\n",
    "        decoder_hidden_states = whisper_outputs.decoder_hidden_states[-1]\n",
    "\n",
    "        # Get turn-end predictions\n",
    "        turn_end_predictions = self.turn_end_classifier(\n",
    "            encoder_hidden_states,\n",
    "            decoder_hidden_states\n",
    "        )\n",
    "        \n",
    "        return whisper_outputs, turn_end_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the CustomWhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnEndClassifier input shapes:\n",
      "- encoder_outputs: torch.Size([2, 1500, 384])\n",
      "- decoder_outputs: torch.Size([2, 20, 384])\n",
      "Output type: <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
      "\n",
      "Available attributes:\n",
      "['clear', 'copy', 'cross_attentions', 'decoder_attentions', 'decoder_hidden_states', 'encoder_attentions', 'encoder_hidden_states', 'encoder_last_hidden_state', 'fromkeys', 'get', 'items', 'keys', 'logits', 'loss', 'move_to_end', 'past_key_values', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n",
      "\n",
      "Key shapes:\n",
      "- logits for word probabilities: torch.Size([2, 20, 51864])\n",
      "- turn_end_predictions: tensor([[0.5960],\n",
      "        [0.5847]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test_custom_whisper_model():\n",
    "    # 1. Create fake data\n",
    "    batch_size = 2\n",
    "    sequence_length = 3000\n",
    "    encoder_dim = 384  # Whisper tiny dimension\n",
    "    decoder_seq_length = 20\n",
    "    \n",
    "    fake_input_features = torch.randn(batch_size, 80, sequence_length)\n",
    "    fake_decoder_input_ids = torch.randint(0, 100, (batch_size, decoder_seq_length))\n",
    "    \n",
    "    # 2. Initialize models\n",
    "    turn_end_classifier = TurnEndClassifier(\n",
    "        encoder_dim=encoder_dim,\n",
    "        decoder_dim=encoder_dim,\n",
    "        hidden_dim=64\n",
    "    )\n",
    "    \n",
    "    model = CustomWhisperModel.from_pretrained(\n",
    "        \"openai/whisper-tiny.en\",\n",
    "        turn_end_classifier=turn_end_classifier\n",
    "    )\n",
    "    \n",
    "    # 3. Run forward pass\n",
    "\n",
    "    whisper_outputs, turn_end_predictions = model.forward_with_turn_end(\n",
    "        input_features=fake_input_features,\n",
    "        decoder_input_ids=fake_decoder_input_ids,\n",
    "    )\n",
    "    print(f\"Output type: {type(whisper_outputs)}\")\n",
    "    print(\"\\nAvailable attributes:\")\n",
    "    print([attr for attr in dir(whisper_outputs) if not attr.startswith('_')])\n",
    "    \n",
    "    print(\"\\nKey shapes:\")\n",
    "    print(f\"- logits for word probabilities: {whisper_outputs.logits.shape}\")\n",
    "    print(f\"- turn_end_predictions: {turn_end_predictions}\")\n",
    "    \n",
    "        \n",
    "test_custom_whisper_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap Custom Model In A Custom Pipeline\n",
    "\n",
    "Create a pipeline that can take a .wav file as input and returns turn_end_probability. \n",
    "\n",
    "As an initial implementation, we will have the pipeline ONLY do prediction for whether the speaker is done speaking (turn-end-classification). The pipeline will not also do transcription.\n",
    "\n",
    "This is computationally inefficient because we have to compute all the encoder and decoder states in order to do turn-end-prediction, and then they will repeated in the Whisper pipeline we use for transcription. A more efficient approach would reuse the encoder and decoder states calculated in the turn-end-prediction as a starting point for transcription, so we only need to do the sampling. But I will save that as a potential future enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurnEndClassifier input shapes:\n",
      "- encoder_outputs: torch.Size([1, 1500, 384])\n",
      "- decoder_outputs: torch.Size([1, 1, 384])\n",
      "Turn end probability: 0.47\n",
      "Time taken: 0.16657567024230957\n",
      "---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Whisper expects the mel input features to be of length 3000, but found 4317. Make sure to pad the input mel features to 3000.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 73\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mturn_end_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlong_audio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTurn end probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturn_end_probability\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36mTurnEndPipeline.__call__\u001b[0;34m(self, audio)\u001b[0m\n\u001b[1;32m     30\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m last_chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Forward pass through model\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m _, turn_end_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_with_turn_end\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturn_end_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m: turn_end_predictions\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m }\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mCustomWhisperModel.forward_with_turn_end\u001b[0;34m(self, input_features, decoder_input_ids, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturn_end_classifier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTurnEndClassifier must be set before calling forward_with_turn_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# whisper_outputs is a Seq2SeqLMOutput object with `logits` for each token, encoder_last_hidden_state, and decoder_hidden_states, decoder_hidden_states, encoder_hidden_states and some other attributes. It's a type of namedtuple.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m whisper_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Get output of last hidden layer from both encoder and decoder.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m whisper_outputs\u001b[38;5;241m.\u001b[39mencoder_last_hidden_state\n",
      "File \u001b[0;32m~/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1767\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1763\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1764\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1765\u001b[0m         )\n\u001b[0;32m-> 1767\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1787\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/interwhisp/iw_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interwhisp/iw_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1618\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1616\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_input_features(input_features, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m-> 1618\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/interwhisp/iw_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/interwhisp/iw_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1018\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1016\u001b[0m expected_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_source_positions \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m expected_seq_length:\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisper expects the mel input features to be of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_seq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Make sure to pad the input mel features to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_seq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m output_attentions \u001b[38;5;241m=\u001b[39m output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_attentions\n\u001b[1;32m   1023\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1024\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1025\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Whisper expects the mel input features to be of length 3000, but found 4317. Make sure to pad the input mel features to 3000."
     ]
    }
   ],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import librosa\n",
    "\n",
    "class TurnEndPipeline(AutomaticSpeechRecognitionPipeline):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_duration = 30.0  # seconds\n",
    "        self.sampling_rate = 16000  # Whisper expects 16kHz\n",
    "    \n",
    "\n",
    "    def _prepare_audio_features(self, audio):\n",
    "        \"\"\"Process audio, taking full audio if <= 30s or last 30s if longer.\"\"\"\n",
    "        audio_array, sampling_rate = librosa.load(audio, sr=self.sampling_rate)\n",
    "        \n",
    "\n",
    "        def _chop_audio(self, audio):\n",
    "            if isinstance(audio, str):\n",
    "                # Load from file\n",
    "                audio_array, sampling_rate = librosa.load(audio, sr=self.sampling_rate)\n",
    "                assert sampling_rate == 16000, f\"Expected sampling rate of 16kHz, but got {sampling_rate}Hz.\"\n",
    "            else:\n",
    "                # Assume audio is already loaded as numpy array\n",
    "                audio_array = audio\n",
    "            max_samples = int(self.max_duration * self.sampling_rate)\n",
    "            return audio_array[-max_samples:]\n",
    "\n",
    "        audio_array = self._chop_audio(audio)\n",
    "        features = self.feature_extractor(\n",
    "            audio_array, \n",
    "            sampling_rate=self.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )        \n",
    "        return features.to(self.device)\n",
    "    \n",
    "    def __call__(self, audio):\n",
    "        \"\"\"Predict if audio ends at a turn boundary.\n",
    "        For files > 30 seconds, only examines the last 30 seconds.\"\"\"\n",
    "        \n",
    "        # Process audio\n",
    "        features = self._prepare_audio_features(audio)\n",
    "        \n",
    "        # Prepare model inputs\n",
    "        model_kwargs = {\n",
    "            \"input_features\": features.input_features,\n",
    "            \"decoder_input_ids\": torch.tensor([[self.model.config.decoder_start_token_id]]).to(self.device)\n",
    "        }\n",
    "        if \"attention_mask\" in features:\n",
    "            model_kwargs[\"attention_mask\"] = features.attention_mask\n",
    "            \n",
    "        # Forward pass through model\n",
    "        _, turn_end_predictions = self.model.forward_with_turn_end(**model_kwargs)\n",
    "        \n",
    "        return {\n",
    "            \"turn_end_probability\": turn_end_predictions.squeeze().item()\n",
    "        }\n",
    "\n",
    "turn_end_classifier = TurnEndClassifier(\n",
    "    encoder_dim=384,\n",
    "    decoder_dim=384,\n",
    "    hidden_dim=64\n",
    ").to(device)\n",
    "\n",
    "\n",
    "turn_end_model = CustomWhisperModel.from_pretrained(\n",
    "    model_id,\n",
    "    turn_end_classifier=turn_end_classifier,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "turn_end_pipe = TurnEndPipeline(\n",
    "    model=turn_end_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "start = time()\n",
    "result = turn_end_pipe(\"./test_data/test.wav\")\n",
    "print(f\"Turn end probability: {result['turn_end_probability']:.2f}\")\n",
    "print(f\"Time taken: {time() - start}\")\n",
    "\n",
    "\n",
    "print(\"---\")\n",
    "start = time()\n",
    "result = turn_end_pipe(long_audio_file)\n",
    "print(f\"Turn end probability: {result['turn_end_probability']:.2f}\")\n",
    "print(f\"Time taken: {time() - start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iw_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
