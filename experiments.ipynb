{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm experimenting with an approach to improve how Spoken Dialog Systems (like OpenAI's Advanced Voice Mode) identify the end of a user's converational turn so the system can jump in appropriately without interrupting the user mid-thought.\n",
    "\n",
    "As a user, current systems appear to rely primarily on pause length from a Voice Activity Detector (VAD). So an `X` second pause mid-sentence is treated similarly to `X` seconds of silence at the end of a sentence.\n",
    "\n",
    "This notebook adds an end-of-turn prediction head to a Whisper model. This prediction head relies on both acoustic and linguistic information (both the Whisper encoder and the Whisper decoder). I will train it from multi-turn conversations between human speakers with labeled transitions. Implicitly, I assume humans do well at knowing when to jump in without interrupting ¯\\_(ツ)_/¯\n",
    "\n",
    "Candidate datasets for training: AMI Meeting Corpus, Switchboard, Fisher English Training Speech, DailyTalk, CALLHOME, CoVoST. If necessary, I could use diarazation tools like pyannotate-audio or pyAudioAnalysis to identify changes in the speaker.\n",
    "\n",
    "A production system might still use a VAD as a computationally cheap approach to identify pauses of at least 1 second, and then I'd call my updated model from this notebook for final end-of-turn detection and transcription only when there is some meaningful pause.\n",
    "\n",
    "This is an educational project to gain initial experience with audio models. I may be naively underestimating what others before me have done. I will nevertheless run incremental experiments as a learning experience. The main steps are:\n",
    "\n",
    "- [x] Download a Whisper model from HuggingFace hub and verify that I can run it on a trivial file\n",
    "- [ ] Inspect the architecture and plan how to integrate a prediction head\n",
    "- [ ] Overfit on a single sample to verify I can train\n",
    "- [ ] Train on a set of ~100 samples with some small number of validation samples to test infrastructure\n",
    "- [ ] Train on a larger sample to test if I can make something that broadly \"works\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Calling A Whisper Model\n",
    "\n",
    "I start with `whisper-tiny.en`. Later stages may use a large model like `whisper-large-v3` once I have the basic workflow wired up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/Users/dan/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "Keys: dict_keys(['text'])\n",
      " My name is Dan and this is a test audio file.\n",
      "Time taken: 0.20616602897644043\n",
      "['__abstractmethods__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_batch_size', '_create_repo', '_ensure_tensor_on_device', '_forward', '_forward_params', '_get_files_timestamps', '_load_feature_extractor', '_load_image_processor', '_load_processor', '_load_tokenizer', '_num_workers', '_postprocess_params', '_preprocess_params', '_sanitize_parameters', '_upload_modified_files', 'binary_output', 'call_count', 'check_model_type', 'default_input_names', 'device', 'device_placement', 'ensure_tensor_on_device', 'feature_extractor', 'forward', 'framework', 'generation_config', 'get_inference_context', 'get_iterator', 'image_processor', 'iterate', 'model', 'modelcard', 'postprocess', 'predict', 'prefix', 'preprocess', 'processor', 'push_to_hub', 'run_multi', 'run_single', 'save_pretrained', 'task', 'tokenizer', 'torch_dtype', 'transform', 'type']\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-tiny.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    " \n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "start = time()\n",
    "result = pipe(\"./test_data/test.wav\")\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Keys: {result.keys() if isinstance(result, dict) else 'not a dict'}\")\n",
    "print(result[\"text\"])\n",
    "print(f\"Time taken: {time() - start}\")\n",
    "print(dir(pipe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Pipeline and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type', 'task', 'model', 'tokenizer', 'feature_extractor', 'image_processor', 'processor', 'modelcard', 'framework', 'device', 'binary_output', 'prefix', 'generation_config', 'call_count']\n"
     ]
    }
   ],
   "source": [
    "# Whats' in the pipeline?\n",
    "print([step for step in pipe.__dict__.keys() if not step.startswith(\"_\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperFeatureExtractor {\n",
       "  \"chunk_length\": 30,\n",
       "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
       "  \"feature_size\": 80,\n",
       "  \"hop_length\": 160,\n",
       "  \"n_fft\": 400,\n",
       "  \"n_samples\": 480000,\n",
       "  \"nb_max_frames\": 3000,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"WhisperProcessor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline expects incoming audio at 16kHz. Preprocessor creates 80 dimensional features in 0.01second increments (the hop length of 160 means 160 frames/samples of audio are combined into a single input frame to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "WhisperForConditionalGeneration                         [1, 1500, 384]            --\n",
      "├─WhisperModel: 1-1                                     [1, 1500, 384]            --\n",
      "│    └─WhisperEncoder: 2-1                              [1, 1500, 384]            576,000\n",
      "│    │    └─Conv1d: 3-1                                 [1, 384, 3000]            92,544\n",
      "│    │    └─Conv1d: 3-2                                 [1, 384, 1500]            442,752\n",
      "│    │    └─ModuleList: 3-3                             --                        7,096,320\n",
      "│    │    └─LayerNorm: 3-4                              [1, 1500, 384]            768\n",
      "│    └─WhisperDecoder: 2-2                              [1, 6, 1, 64]             --\n",
      "│    │    └─Embedding: 3-5                              [1, 1, 384]               19,915,776\n",
      "│    │    └─WhisperPositionalEmbedding: 3-6             [1, 1, 384]               172,032\n",
      "│    │    └─ModuleList: 3-7                             --                        9,463,296\n",
      "│    │    └─LayerNorm: 3-8                              [1, 1, 384]               768\n",
      "├─Linear: 1-2                                           [1, 1, 51864]             19,915,776\n",
      "=========================================================================================================\n",
      "Total params: 57,676,032\n",
      "Trainable params: 57,100,032\n",
      "Non-trainable params: 576,000\n",
      "Total mult-adds (Units.MEGABYTES): 998.32\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.96\n",
      "Forward/backward pass size (MB): 258.64\n",
      "Params size (MB): 228.40\n",
      "Estimated Total Size (MB): 488.00\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "decoder_input_ids = torch.tensor([[processor.tokenizer.pad_token_id]]).to(device)\n",
    "print(summary(pipe.model, input_size=(1, 80, 3000), decoder_input_ids=decoder_input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding TurnEndClassifier to Whisper Model\n",
    "\n",
    "I add a TurnEndClassifier as an additional prediction head for the Whisper model. It's inputs are \n",
    "1. audio features from the encoder\n",
    "2. Semantic info from decoder\n",
    "\n",
    "## Alignment\n",
    "\n",
    "Each recording will be classified with a single prediction of whether it ends at a turn-end. The recording has many values from the encoder on the time dimension (100 samples per second) and many values from the decoder (1 hidden state per token).\n",
    "\n",
    "I compress the time and token dimensions from the encoder and decoder respectively into a 1d representation from each. I do these compressions with convolutional layers and then pooling layers (separate layers for the encoder and the decoder compression to 1d). These 1d vectors are concatenated, fed through a small feedforward network, and lead to a binary classification head indicating if this audio finishes with a turn-end.\n",
    "\n",
    "For training, I'll preprocess conversations to have many samples that are each 10 seconds of audio and that finish at a turn end (target is 1) or that don't (target is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class TurnEndClassifier(nn.Module):\n",
    "    def __init__(self, encoder_dim=384, decoder_dim=384, hidden_dim=64):\n",
    "        super(TurnEndClassifier, self).__init__()\n",
    "\n",
    "        # Linear layers to reduce the dimensions of the encoder and decoder outputs.\n",
    "        # This is a temporary hack. In theory, hidden_dim could be larger than whisper_hidden_dim. Or encoder and decoder could have different dimensions.\n",
    "        self.encoder_reduce = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.decoder_reduce = nn.Linear(decoder_dim, hidden_dim)\n",
    "        \n",
    "        # Encoder processing layers\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.encoder_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Decoder processing layers\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim , hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_outputs):\n",
    "        print(f\"TurnEndClassifier input shapes:\")\n",
    "        print(f\"- encoder_outputs: {encoder_outputs.shape}\")  # [batch, time, features]\n",
    "        print(f\"- decoder_outputs: {decoder_outputs.shape}\")  # [batch, seq_len, features]\n",
    "\n",
    "        # Reduce feature dimension first (more efficient for subsequent conv layers)\n",
    "        # [batch, time, whisper_hidden_dim] -> [batch, time, hidden_dim]\n",
    "        encoder_outputs = self.encoder_reduce(encoder_outputs)\n",
    "        \n",
    "        # [batch, seq_len, whisper_hidden_dim] -> [batch, seq_len, hidden_dim]\n",
    "        decoder_outputs = self.decoder_reduce(decoder_outputs)\n",
    "        \n",
    "        # Process encoder outputs\n",
    "        # [batch, time, hidden_dim] -> [batch, hidden_dim, time]\n",
    "        encoder_outputs = encoder_outputs.transpose(1, 2)  # Conv1d expects channels first\n",
    "        encoder_features = self.encoder_conv(encoder_outputs)  # Now working with reduced dimensions\n",
    "        encoder_features = self.encoder_pool(encoder_features).squeeze(-1)  # -> [batch, hidden_dim]\n",
    "        \n",
    "        # Process decoder outputs\n",
    "        # [batch, seq_len, hidden_dim] -> [batch, hidden_dim, seq_len]\n",
    "        decoder_outputs = decoder_outputs.transpose(1, 2)  # Conv1d expects channels first\n",
    "        decoder_features = self.decoder_conv(decoder_outputs)  # Now working with reduced dimensions\n",
    "        decoder_features = self.decoder_pool(decoder_features).squeeze(-1)  # -> [batch, hidden_dim]\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        # [batch, hidden_dim] + [batch, hidden_dim] -> [batch, 2*hidden_dim]\n",
    "        combined_features = torch.cat((encoder_features, decoder_features), dim=1)\n",
    "        # [batch, 2*hidden_dim] -> [batch, 1]\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model\n",
    "Now we build the custom model that is a Whisper Model with the extra prediction head. Our goals include:\n",
    "1. Allow training the layers in the TurnEndClassifier prediction head while keeping all other layers frozen\n",
    "2. Allow embedding this in a pipeline that is as similar as possible to the pipeline used at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomWhisperModel(WhisperForConditionalGeneration):\n",
    "    def __init__(self, config, turn_end_classifier=None):\n",
    "        super().__init__(config)\n",
    "        self.turn_end_classifier = turn_end_classifier # if turn_end_classifier is None, we'll set it later\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, turn_end_classifier, *args, **kwargs):\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n",
    "        model.turn_end_classifier = turn_end_classifier\n",
    "        return model\n",
    "\n",
    "    def forward_with_turn_end(\n",
    "        self,\n",
    "        input_features=None,\n",
    "        decoder_input_ids=None,\n",
    "        **kwargs  # Catch all other args to pass through\n",
    "    ):\n",
    "        assert self.turn_end_classifier is not None, \"TurnEndClassifier must be set before calling forward_with_turn_end\"\n",
    "        \n",
    "        # whisper_outputs is a Seq2SeqLMOutput object with `logits` for each token, encoder_last_hidden_state, and decoder_hidden_states, decoder_hidden_states, encoder_hidden_states and some other attributes. It's a type of namedtuple.\n",
    "        whisper_outputs = super().forward(\n",
    "            input_features=input_features,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            output_hidden_states=True,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Get output of last hidden layer from both encoder and decoder.\n",
    "        encoder_hidden_states = whisper_outputs.encoder_last_hidden_state\n",
    "        decoder_hidden_states = whisper_outputs.decoder_hidden_states[-1]\n",
    "\n",
    "        # Get turn-end predictions\n",
    "        turn_end_predictions = self.turn_end_classifier(\n",
    "            encoder_hidden_states,\n",
    "            decoder_hidden_states\n",
    "        )\n",
    "        \n",
    "        return whisper_outputs, turn_end_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the CustomWhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TurnEndClassifier with dims: encoder=384, decoder=384\n",
      "TurnEndClassifier input shapes:\n",
      "- encoder_outputs: torch.Size([2, 1500, 384])\n",
      "- decoder_outputs: torch.Size([2, 20, 384])\n",
      "Output type: <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
      "\n",
      "Available attributes:\n",
      "['clear', 'copy', 'cross_attentions', 'decoder_attentions', 'decoder_hidden_states', 'encoder_attentions', 'encoder_hidden_states', 'encoder_last_hidden_state', 'fromkeys', 'get', 'items', 'keys', 'logits', 'loss', 'move_to_end', 'past_key_values', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n",
      "\n",
      "Key shapes:\n",
      "- logits for word probabilities: torch.Size([2, 20, 51864])\n",
      "- turn_end_predictions: tensor([[0.5481],\n",
      "        [0.5333]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test_custom_whisper_model():\n",
    "    # 1. Create fake data\n",
    "    batch_size = 2\n",
    "    sequence_length = 3000\n",
    "    encoder_dim = 384  # Whisper tiny dimension\n",
    "    decoder_seq_length = 20\n",
    "    \n",
    "    fake_input_features = torch.randn(batch_size, 80, sequence_length)\n",
    "    fake_decoder_input_ids = torch.randint(0, 100, (batch_size, decoder_seq_length))\n",
    "    \n",
    "    # 2. Initialize models\n",
    "    turn_end_classifier = TurnEndClassifier(\n",
    "        encoder_dim=encoder_dim,\n",
    "        decoder_dim=encoder_dim,\n",
    "        hidden_dim=64\n",
    "    )\n",
    "    \n",
    "    model = CustomWhisperModel.from_pretrained(\n",
    "        \"openai/whisper-tiny.en\",\n",
    "        turn_end_classifier=turn_end_classifier\n",
    "    )\n",
    "    \n",
    "    # 3. Run forward pass\n",
    "\n",
    "    whisper_outputs, turn_end_predictions = model.forward_with_turn_end(\n",
    "        input_features=fake_input_features,\n",
    "        decoder_input_ids=fake_decoder_input_ids,\n",
    "    )\n",
    "    print(f\"Output type: {type(whisper_outputs)}\")\n",
    "    print(\"\\nAvailable attributes:\")\n",
    "    print([attr for attr in dir(whisper_outputs) if not attr.startswith('_')])\n",
    "    \n",
    "    print(\"\\nKey shapes:\")\n",
    "    print(f\"- logits for word probabilities: {whisper_outputs.logits.shape}\")\n",
    "    print(f\"- turn_end_predictions: {turn_end_predictions}\")\n",
    "    \n",
    "        \n",
    "test_custom_whisper_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap Custom Model In A Custom Pipeline\n",
    "\n",
    "Create a pipeline that can take a .wav file as input and return both the text and turn_end_probability. After this is working, I will later explore adding functionality to run this without doing the sampling of tokens. Sampling is autoregressive, so it may be slow. And could theoretically run just end-of-turn classification, and then get the transcription iff we find it is in fact the end of turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TurnEndClassifier with dims: encoder=384, decoder=384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 96\u001b[0m\n\u001b[1;32m     87\u001b[0m pipe \u001b[38;5;241m=\u001b[39m TurnEndPipeline(\n\u001b[1;32m     88\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     89\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Two-stage usage\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m result, context \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_turn_end\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./test_data/test.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTurn end probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturn_end_probability\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mturn_end_probability\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m:  \u001b[38;5;66;03m# or whatever threshold you choose\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mTurnEndPipeline.predict_turn_end\u001b[0;34m(self, audio_input)\u001b[0m\n\u001b[1;32m     23\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(audio_input)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Get turn-end prediction and cache states\u001b[39;00m\n\u001b[1;32m     26\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmodel_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id]])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     29\u001b[0m }\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_inputs:\n\u001b[1;32m     31\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "import torch\n",
    "from enum import Enum\n",
    "\n",
    "@dataclass\n",
    "class TurnEndContext:\n",
    "    \"\"\"Context object holding state between turn-end prediction and transcription\"\"\"\n",
    "    encoder_outputs: torch.Tensor\n",
    "    decoder_outputs: torch.Tensor\n",
    "    attention_mask: Optional[torch.Tensor]\n",
    "\n",
    "class TurnEndPipeline(AutomaticSpeechRecognitionPipeline):\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise ValueError(\n",
    "            \"This pipeline uses a two-stage approach. Please use predict_turn_end() \"\n",
    "            \"followed by transcribe() instead of calling the pipeline directly\"\n",
    "        )\n",
    "    def predict_turn_end(self, audio_input):\n",
    "        \"\"\"First stage: Predict if audio ends at a turn boundary.\"\"\"\n",
    "        model_inputs_generator = self.preprocess(audio_input)\n",
    "        \n",
    "        results = []\n",
    "        contexts = []\n",
    "        \n",
    "        # Process each chunk/batch item\n",
    "        for model_inputs in model_inputs_generator:\n",
    "            # Get turn-end prediction and cache states\n",
    "            model_kwargs = {\n",
    "                \"input_features\": model_inputs[\"input_features\"],\n",
    "                \"decoder_input_ids\": torch.tensor([[self.model.config.decoder_start_token_id]]).to(self.device)\n",
    "            }\n",
    "            if \"attention_mask\" in model_inputs:\n",
    "                model_kwargs[\"attention_mask\"] = model_inputs[\"attention_mask\"]\n",
    "\n",
    "            # Forward pass through our custom Whisper model\n",
    "            whisper_outputs, turn_end_predictions = self.model.forward_with_turn_end(**model_kwargs)\n",
    "            \n",
    "            # Create context object for potential transcription\n",
    "            contexts.append(TurnEndContext(\n",
    "                encoder_outputs=whisper_outputs.encoder_last_hidden_state,\n",
    "                decoder_outputs=whisper_outputs.decoder_hidden_states[-1],\n",
    "                attention_mask=model_inputs.get(\"attention_mask\", None)\n",
    "            ))\n",
    "            \n",
    "            results.append({\n",
    "                \"turn_end_probability\": turn_end_predictions.squeeze().item()\n",
    "            })\n",
    "        \n",
    "        # For now, just return the first result/context\n",
    "        # TODO: Handle multiple chunks/batches appropriately\n",
    "        return results[0], contexts[0]\n",
    "\n",
    "\n",
    "    def transcribe(self, context: TurnEndContext):\n",
    "        \"\"\"Second stage: Generate transcription using context from first stage.\"\"\"\n",
    "        if not isinstance(context, TurnEndContext):\n",
    "            raise ValueError(\n",
    "                \"Transcribe requires a TurnEndContext object from predict_turn_end(). \"\n",
    "                \"Please call predict_turn_end() first.\"\n",
    "            )\n",
    "            \n",
    "        # Use cached states to generate transcription\n",
    "        generated_ids = self.model.generate(\n",
    "            encoder_outputs=context.encoder_outputs,\n",
    "            attention_mask=context.attention_mask\n",
    "        )\n",
    "        \n",
    "        # Decode the generated IDs to text\n",
    "        text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        \n",
    "        return {\"text\": text}\n",
    "# Create the turn end classifier\n",
    "turn_end_classifier = TurnEndClassifier(\n",
    "    encoder_dim=384,  # Whisper tiny dimension\n",
    "    decoder_dim=384,\n",
    "    hidden_dim=64\n",
    ").to(device)\n",
    "\n",
    "# Create the custom model\n",
    "model = CustomWhisperModel.from_pretrained(\n",
    "    model_id,\n",
    "    turn_end_classifier=turn_end_classifier,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "# Create the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = TurnEndPipeline(\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Two-stage usage\n",
    "result, context = pipe.predict_turn_end(\"./test_data/test.wav\")\n",
    "print(f\"Turn end probability: {result['turn_end_probability']:.2f}\")\n",
    "\n",
    "if result['turn_end_probability'] > 0.5:  # or whatever threshold you choose\n",
    "    transcription = pipe.transcribe(context)\n",
    "    print(f\"Transcription: {transcription['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iw_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
