{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm experimenting with an approach to improve how Spoken Dialog Systems (like OpenAI's Advanced Voice Mode) identify when a user is done speaking (the end of a their `converational turn`) so it responds at appropriate times without interrupting the user mid-thought.\n",
    "\n",
    "As a user, current systems appear to rely primarily on pause length from a Voice Activity Detector (VAD). So an `X` second pause mid-sentence is treated similarly to `X` seconds of silence at the end of a sentence.\n",
    "\n",
    "This notebook adds an end-of-turn prediction head to a Whisper model. This prediction head relies on both acoustic and linguistic information (both the Whisper encoder and the Whisper decoder). I will train it from multi-turn conversations between human speakers with labeled transitions. Implicitly, I assume humans do well at knowing when to jump in without interrupting ¯\\_(ツ)_/¯\n",
    "\n",
    "Candidate datasets for training: AMI Meeting Corpus, Switchboard (which has a $3k licensing fee), CALLHOME. If necessary, I could use diarazation tools like pyannotate-audio or pyAudioAnalysis to identify changes in the speaker.\n",
    "\n",
    "A production system might still use a VAD as a computationally cheap approach to identify pauses of at least 1 second, and then I'd call my updated model from this notebook for final end-of-turn detection and transcription only when there is some meaningful pause.\n",
    "\n",
    "This is an educational project to gain initial experience with audio models. I may be naively underestimating what others before me have done. I will nevertheless run incremental experiments as a learning experience. The main steps are:\n",
    "\n",
    "- [x] Download a Whisper model from HuggingFace hub and verify that I can run it on a trivial file\n",
    "- [x] Inspect the architecture and plan how to integrate a prediction head\n",
    "- [x] Overfit on a single sample to verify I can train\n",
    "- [ ] Train on a set of ~100 samples with some small number of validation samples to test infrastructure\n",
    "- [ ] Train on a larger sample to test if I can make something that broadly \"works\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Calling A Whisper Model\n",
    "\n",
    "I start with `whisper-tiny.en`. Later stages may use a large model like `whisper-large-v3` once I have the basic workflow wired up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/Users/dan/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['text'])\n",
      " My name is Dan and this is a test audio file.\n",
      "Time taken: 0.24007105827331543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/interwhisp/iw_venv/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" Okay, this is going to be a test of recordings that are longer than 30 seconds so that I can test how well this model handles multi chunk. Audio files. So each chunk is going to be 30 seconds. If you have a recording this longer than 30 seconds, it's going to be processed as multiple chunks. And then each chunk is going to be handled by the large-engaged model to make a transcription. And then we're going to concatenate those various transcriptions. .\", 'chunks': [{'timestamp': (0.0, 15.0), 'text': ' Okay, this is going to be a test of recordings that are longer than 30 seconds so that I can test how well this model handles multi chunk.'}, {'timestamp': (15.0, 0.0), 'text': ''}, {'timestamp': (7.0, 14.0), 'text': ' Audio files. So each chunk is going to be 30 seconds.'}, {'timestamp': (14.0, 18.0), 'text': \" If you have a recording this longer than 30 seconds, it's going to be processed as multiple chunks.\"}, {'timestamp': (18.0, 23.0), 'text': ' And then each chunk is going to be handled by the large-engaged model to make a transcription.'}, {'timestamp': (23.0, 28.0), 'text': \" And then we're going to concatenate those various transcriptions.\"}, {'timestamp': (28.0, 0.0), 'text': ''}, {'timestamp': (2.0, None), 'text': ' .'}]}\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-tiny.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    " \n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "start = time()\n",
    "result = pipe(\"./test_data/test.wav\")\n",
    "print(f\"Keys: {result.keys() if isinstance(result, dict) else 'not a dict'}\")\n",
    "print(result[\"text\"])\n",
    "print(f\"Time taken: {time() - start}\")\n",
    "\n",
    "long_audio_file = \"./test_data/multi-chunk-test.wav\"\n",
    "result2 = pipe(long_audio_file, return_timestamps=True)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Pipeline and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type', 'task', 'model', 'tokenizer', 'feature_extractor', 'image_processor', 'processor', 'modelcard', 'framework', 'device', 'binary_output', 'prefix', 'generation_config', 'call_count']\n"
     ]
    }
   ],
   "source": [
    "# Whats' in the pipeline?\n",
    "print([step for step in pipe.__dict__.keys() if not step.startswith(\"_\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperFeatureExtractor {\n",
       "  \"chunk_length\": 30,\n",
       "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
       "  \"feature_size\": 80,\n",
       "  \"hop_length\": 160,\n",
       "  \"n_fft\": 400,\n",
       "  \"n_samples\": 480000,\n",
       "  \"nb_max_frames\": 3000,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"WhisperProcessor\",\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline expects incoming audio at 16kHz. Preprocessor creates 80 dimensional features in 0.01second increments (the hop length of 160 means 160 frames/samples of audio are combined into a single input frame to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "WhisperForConditionalGeneration                         [1, 1500, 384]            --\n",
      "├─WhisperModel: 1-1                                     [1, 1500, 384]            --\n",
      "│    └─WhisperEncoder: 2-1                              [1, 1500, 384]            576,000\n",
      "│    │    └─Conv1d: 3-1                                 [1, 384, 3000]            92,544\n",
      "│    │    └─Conv1d: 3-2                                 [1, 384, 1500]            442,752\n",
      "│    │    └─ModuleList: 3-3                             --                        7,096,320\n",
      "│    │    └─LayerNorm: 3-4                              [1, 1500, 384]            768\n",
      "│    └─WhisperDecoder: 2-2                              [1, 6, 1, 64]             --\n",
      "│    │    └─Embedding: 3-5                              [1, 1, 384]               19,915,776\n",
      "│    │    └─WhisperPositionalEmbedding: 3-6             [1, 1, 384]               172,032\n",
      "│    │    └─ModuleList: 3-7                             --                        9,463,296\n",
      "│    │    └─LayerNorm: 3-8                              [1, 1, 384]               768\n",
      "├─Linear: 1-2                                           [1, 1, 51864]             19,915,776\n",
      "=========================================================================================================\n",
      "Total params: 57,676,032\n",
      "Trainable params: 57,100,032\n",
      "Non-trainable params: 576,000\n",
      "Total mult-adds (Units.MEGABYTES): 998.32\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.96\n",
      "Forward/backward pass size (MB): 258.64\n",
      "Params size (MB): 228.40\n",
      "Estimated Total Size (MB): 488.00\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "decoder_input_ids = torch.tensor([[processor.tokenizer.pad_token_id]]).to(device)\n",
    "print(summary(pipe.model, input_size=(1, 80, 3000), decoder_input_ids=decoder_input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding TurnEndClassifier to Whisper Model\n",
    "\n",
    "I add a TurnEndClassifier as an additional prediction head for the Whisper model. It's inputs are \n",
    "1. audio features from the encoder\n",
    "2. Semantic info from decoder\n",
    "\n",
    "## Alignment\n",
    "\n",
    "Each recording will be classified with a single prediction of whether it ends at a turn-end. The recording has many values from the encoder on the time dimension (100 samples per second) and many values from the decoder (1 hidden state per token).\n",
    "\n",
    "I compress the time and token dimensions from the encoder and decoder respectively into a 1d representation from each. I do these compressions with convolutional layers and then pooling layers (separate layers for the encoder and the decoder compression to 1d). These 1d vectors are concatenated, fed through a small feedforward network, and lead to a binary classification head indicating if this audio finishes with a turn-end.\n",
    "\n",
    "For training, I'll preprocess conversations to have many samples that are each 10 seconds of audio and that finish at a turn end (target is 1) or that don't (target is 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class TurnEndClassifier(nn.Module):\n",
    "    def __init__(self, encoder_dim=384, decoder_dim=384, hidden_dim=64):\n",
    "        super(TurnEndClassifier, self).__init__()\n",
    "\n",
    "        # Linear layers to reduce the dimensions of the encoder and decoder outputs.\n",
    "        # This is a temporary hack. In theory, hidden_dim could be larger than whisper_hidden_dim. Or encoder and decoder could have different dimensions.\n",
    "        self.encoder_reduce = nn.Linear(encoder_dim, hidden_dim)\n",
    "        self.decoder_reduce = nn.Linear(decoder_dim, hidden_dim)\n",
    "        \n",
    "        # Encoder processing layers\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=5, stride=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.encoder_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Decoder processing layers\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim , hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_outputs):\n",
    "        # Reduce feature dimension first (more efficient for subsequent conv layers)\n",
    "        # [batch, time, whisper_hidden_dim] -> [batch, time, hidden_dim]\n",
    "        encoder_outputs = self.encoder_reduce(encoder_outputs)\n",
    "        \n",
    "        # [batch, seq_len, whisper_hidden_dim] -> [batch, seq_len, hidden_dim]\n",
    "        decoder_outputs = self.decoder_reduce(decoder_outputs)\n",
    "        \n",
    "        # Process encoder outputs\n",
    "        # [batch, time, hidden_dim] -> [batch, hidden_dim, time]\n",
    "        encoder_outputs = encoder_outputs.transpose(1, 2)  # Conv1d expects channels first\n",
    "        encoder_features = self.encoder_conv(encoder_outputs)  # Now working with reduced dimensions\n",
    "        encoder_features = self.encoder_pool(encoder_features).squeeze(-1)  # -> [batch, hidden_dim]\n",
    "        \n",
    "        # Process decoder outputs\n",
    "        # [batch, seq_len, hidden_dim] -> [batch, hidden_dim, seq_len]\n",
    "        decoder_outputs = decoder_outputs.transpose(1, 2)  # Conv1d expects channels first\n",
    "        decoder_features = self.decoder_conv(decoder_outputs)  # Now working with reduced dimensions\n",
    "        decoder_features = self.decoder_pool(decoder_features).squeeze(-1)  # -> [batch, hidden_dim]\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        # [batch, hidden_dim] + [batch, hidden_dim] -> [batch, 2*hidden_dim]\n",
    "        combined_features = torch.cat((encoder_features, decoder_features), dim=1)\n",
    "        # [batch, 2*hidden_dim] -> [batch, 1]\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model\n",
    "Now we build the custom model that is a Whisper Model with the extra prediction head. Our goals include:\n",
    "1. Allow training the layers in the TurnEndClassifier prediction head while keeping all other layers frozen\n",
    "2. Allow embedding this in a pipeline that reuses parts of the Whisper pipeline (e.g. for preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomWhisperModel(WhisperForConditionalGeneration):\n",
    "    def __init__(self, config, turn_end_classifier=None):\n",
    "        super().__init__(config)\n",
    "        self.turn_end_classifier = turn_end_classifier # if turn_end_classifier is None, we'll set it later\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, turn_end_classifier, *args, **kwargs):\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *args, **kwargs)\n",
    "        model.turn_end_classifier = turn_end_classifier\n",
    "        return model\n",
    "\n",
    "    def forward_with_turn_end(\n",
    "        self,\n",
    "        input_features=None,\n",
    "        decoder_input_ids=None,\n",
    "        **kwargs  # Catch all other args to pass through\n",
    "    ):\n",
    "        assert self.turn_end_classifier is not None, \"TurnEndClassifier must be set before calling forward_with_turn_end\"\n",
    "        \n",
    "        # whisper_outputs is a Seq2SeqLMOutput object with `logits` for each token, encoder_last_hidden_state, and decoder_hidden_states, decoder_hidden_states, encoder_hidden_states and some other attributes. It's a type of namedtuple.\n",
    "        whisper_outputs = super().forward(\n",
    "            input_features=input_features,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            output_hidden_states=True,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Get output of last hidden layer from both encoder and decoder.\n",
    "        encoder_hidden_states = whisper_outputs.encoder_last_hidden_state\n",
    "        decoder_hidden_states = whisper_outputs.decoder_hidden_states[-1]\n",
    "\n",
    "        turn_end_predictions = self.turn_end_classifier(\n",
    "            encoder_hidden_states,\n",
    "            decoder_hidden_states\n",
    "        )\n",
    "        \n",
    "        return whisper_outputs, turn_end_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the CustomWhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output type: <class 'transformers.modeling_outputs.Seq2SeqLMOutput'>\n",
      "\n",
      "Available attributes:\n",
      "['clear', 'copy', 'cross_attentions', 'decoder_attentions', 'decoder_hidden_states', 'encoder_attentions', 'encoder_hidden_states', 'encoder_last_hidden_state', 'fromkeys', 'get', 'items', 'keys', 'logits', 'loss', 'move_to_end', 'past_key_values', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    "def test_custom_whisper_model():\n",
    "    # 1. Create fake data\n",
    "    batch_size = 2\n",
    "    sequence_length = 3000\n",
    "    encoder_dim = 384  # Whisper tiny dimension\n",
    "    decoder_seq_length = 20\n",
    "    \n",
    "    fake_input_features = torch.randn(batch_size, 80, sequence_length)\n",
    "    fake_decoder_input_ids = torch.randint(0, 100, (batch_size, decoder_seq_length))\n",
    "    \n",
    "    # 2. Initialize models\n",
    "    turn_end_classifier = TurnEndClassifier(\n",
    "        encoder_dim=encoder_dim,\n",
    "        decoder_dim=encoder_dim,\n",
    "        hidden_dim=64\n",
    "    )\n",
    "    \n",
    "    model = CustomWhisperModel.from_pretrained(\n",
    "        \"openai/whisper-tiny.en\",\n",
    "        turn_end_classifier=turn_end_classifier\n",
    "    )\n",
    "    \n",
    "    # 3. Run forward pass\n",
    "\n",
    "    whisper_outputs, turn_end_predictions = model.forward_with_turn_end(\n",
    "        input_features=fake_input_features,\n",
    "        decoder_input_ids=fake_decoder_input_ids,\n",
    "    )\n",
    "    print(f\"Output type: {type(whisper_outputs)}\")\n",
    "    print(\"\\nAvailable attributes:\")\n",
    "    print([attr for attr in dir(whisper_outputs) if not attr.startswith('_')])\n",
    "    \n",
    "    \n",
    "        \n",
    "test_custom_whisper_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap Custom Model In A Custom Pipeline\n",
    "\n",
    "Create a pipeline that can take a .wav file as input and returns turn_end_probability. \n",
    "\n",
    "As an initial implementation, we will have the pipeline ONLY do prediction for whether the speaker is done speaking (turn-end-classification). The pipeline will not also do transcription.\n",
    "\n",
    "This is computationally inefficient because we have to compute all the encoder and decoder states in order to do turn-end-prediction, and then they will repeated in the Whisper pipeline we use for transcription. A more efficient approach would reuse the encoder and decoder states calculated in the turn-end-prediction as a starting point for transcription, so we only need to do the sampling. But I will save that as a potential future enhancement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn end probability: 0.53\n",
      "Time taken: 0.47891807556152344\n",
      "---\n",
      "Turn end probability: 0.52\n",
      "Time taken: 0.09000015258789062\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import librosa\n",
    "\n",
    "class TurnEndPipeline(AutomaticSpeechRecognitionPipeline):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_duration = 30.0  # seconds\n",
    "        self.sampling_rate = 16000  # Whisper expects 16kHz\n",
    "    \n",
    "\n",
    "    def _prepare_audio_features(self, audio):\n",
    "        \"\"\"Process audio, taking full audio if <= 30s or last 30s if longer.\"\"\"\n",
    "\n",
    "        if isinstance(audio, str):\n",
    "            # Load from file\n",
    "            audio_array, sampling_rate = librosa.load(audio, sr=self.sampling_rate)\n",
    "            assert sampling_rate == 16000, f\"Expected sampling rate of 16kHz, but got {sampling_rate}Hz.\"\n",
    "        else:\n",
    "            # Assume audio is already loaded as numpy array\n",
    "            audio_array = audio\n",
    "        max_samples = int(self.max_duration * self.sampling_rate)\n",
    "        audio_array = audio_array[-max_samples:]\n",
    "\n",
    "        features = self.feature_extractor(\n",
    "            audio_array, \n",
    "            sampling_rate=self.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )        \n",
    "        return features.to(self.device)\n",
    "    \n",
    "    def __call__(self, audio):\n",
    "        \"\"\"Predict if audio ends at a turn boundary.\n",
    "        For files > 30 seconds, only examines the last 30 seconds.\"\"\"\n",
    "        \n",
    "        # Process audio\n",
    "        features = self._prepare_audio_features(audio)\n",
    "        \n",
    "        # Prepare model inputs\n",
    "        model_kwargs = {\n",
    "            \"input_features\": features.input_features,\n",
    "            \"decoder_input_ids\": torch.tensor([[self.model.config.decoder_start_token_id]]).to(self.device)\n",
    "        }\n",
    "        if \"attention_mask\" in features:\n",
    "            model_kwargs[\"attention_mask\"] = features.attention_mask\n",
    "            \n",
    "        # Forward pass through model\n",
    "        _, turn_end_predictions = self.model.forward_with_turn_end(**model_kwargs)\n",
    "        \n",
    "        return {\n",
    "            \"turn_end_probability\": turn_end_predictions.squeeze().item()\n",
    "        }\n",
    "\n",
    "turn_end_classifier = TurnEndClassifier(\n",
    "    encoder_dim=384,\n",
    "    decoder_dim=384,\n",
    "    hidden_dim=64\n",
    ").to(device)\n",
    "\n",
    "\n",
    "turn_end_model = CustomWhisperModel.from_pretrained(\n",
    "    model_id,\n",
    "    turn_end_classifier=turn_end_classifier,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "turn_end_pipe = TurnEndPipeline(\n",
    "    model=turn_end_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "start = time()\n",
    "result = turn_end_pipe(\"./test_data/test.wav\")\n",
    "print(f\"Turn end probability: {result['turn_end_probability']:.2f}\")\n",
    "print(f\"Time taken: {time() - start}\")\n",
    "\n",
    "\n",
    "print(\"---\")\n",
    "start = time()\n",
    "result = turn_end_pipe(long_audio_file)\n",
    "print(f\"Turn end probability: {result['turn_end_probability']:.2f}\")\n",
    "print(f\"Time taken: {time() - start}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Overfit on Single Sample\n",
    "Just to test that we can do basic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/51 [00:00<00:25,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7394\n",
      "Target: 1.0, Prediction: 0.5481\n",
      "Target: 0.0, Prediction: 0.5481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/51 [00:04<00:15,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.6957\n",
      "Target: 1.0, Prediction: 0.5067\n",
      "Target: 0.0, Prediction: 0.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 21/51 [00:07<00:11,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.6903\n",
      "Target: 1.0, Prediction: 0.5034\n",
      "Target: 0.0, Prediction: 0.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 31/51 [00:11<00:08,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.6219\n",
      "Target: 1.0, Prediction: 0.5919\n",
      "Target: 0.0, Prediction: 0.4749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 41/51 [00:15<00:04,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.0011\n",
      "Target: 1.0, Prediction: 0.9994\n",
      "Target: 0.0, Prediction: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:19<00:00,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.0000\n",
      "Target: 1.0, Prediction: 1.0000\n",
      "Target: 0.0, Prediction: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_training_samples(audio_path):\n",
    "    \"\"\"Create two samples from the audio file\"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    # Sample 1: Full audio (turn end)\n",
    "    full_audio = audio\n",
    "    \n",
    "    # Sample 2: First 4 seconds (not turn end)\n",
    "    partial_audio = audio[:int(4 * sr)]\n",
    "    \n",
    "    return full_audio, partial_audio\n",
    "\n",
    "def train_single_epoch(model, samples, labels, feature_extractor, tokenizer, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch on our two samples\"\"\"\n",
    "    model.eval()  # Freeze Whisper\n",
    "    model.turn_end_classifier.train()  # Train only classifier\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for audio, label in zip(samples, labels):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Preprocess audio\n",
    "        features = feature_extractor(\n",
    "            audio, \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Prepare model inputs\n",
    "        decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]]).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        _, turn_end_pred = model.forward_with_turn_end(\n",
    "            input_features=features.input_features,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "        \n",
    "        # Calculate loss\n",
    "        target = torch.tensor([[label]], dtype=torch.float).to(device)\n",
    "        loss = criterion(turn_end_pred, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(samples)\n",
    "\n",
    "def train_model(audio_path, model, feature_extractor, tokenizer, device, num_epochs=51):\n",
    "    \"\"\"Main training loop\"\"\"\n",
    "    # Prepare samples\n",
    "    full_audio, partial_audio = create_training_samples(audio_path)\n",
    "    samples = [full_audio, partial_audio]\n",
    "    labels = [1.0, 0.0]  # 1 for turn end, 0 for not turn end\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = optim.Adam(model.turn_end_classifier.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        loss = train_single_epoch(\n",
    "            model, samples, labels, \n",
    "            feature_extractor, tokenizer, \n",
    "            optimizer, criterion, device\n",
    "        )\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            # Print predictions\n",
    "            with torch.no_grad():\n",
    "                for audio, label in zip(samples, labels):\n",
    "                    features = feature_extractor(\n",
    "                        audio, \n",
    "                        sampling_rate=16000, \n",
    "                        return_tensors=\"pt\"\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]]).to(device)\n",
    "                    _, pred = model.forward_with_turn_end(\n",
    "                        input_features=features.input_features,\n",
    "                        decoder_input_ids=decoder_input_ids\n",
    "                    )\n",
    "                    print(f\"Target: {label:.1f}, Prediction: {pred.item():.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Run training\n",
    "losses = train_model(\n",
    "    \"./test_data/test.wav\",\n",
    "    turn_end_model,\n",
    "    turn_end_pipe.feature_extractor,\n",
    "    turn_end_pipe.tokenizer,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iw_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
